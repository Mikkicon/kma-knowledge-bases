{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: selenium in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (4.14.0)\n",
      "Collecting webdriver_manager\n",
      "  Obtaining dependency information for webdriver_manager from https://files.pythonhosted.org/packages/b1/51/b5c11cf739ac4eecde611794a0ec9df420d0239d51e73bc19eb44f02b48b/webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from selenium) (2.0.4)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: requests in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from webdriver_manager) (23.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from requests->webdriver_manager) (3.2.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mp/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install bs4 selenium webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import importlib  \n",
    "try:\n",
    "    constants = importlib.import_module(\"knowledge-bases.hw1.constants\")\n",
    "except:\n",
    "    print('')\n",
    "from constants import get_output_paths\n",
    "\n",
    "def fill_job_template(url=None, company_name=None, job_title=None, salary=None, location=None, description=None, views=None, responses=None, category=None, employment=None, experience=None, english=None, domain=None):\n",
    "    return  {\n",
    "        \"URL\" : url if url else \"\",\n",
    "        \"Company\" : company_name if company_name else \"\",\n",
    "        \"Job\" : job_title if job_title else \"\", \n",
    "        \"Salary\" : salary if salary else \"\",\n",
    "        \"Location\" : location if location else \"\",\n",
    "        \"Views\" : views if views else \"\",\n",
    "        \"Responses\" : responses if responses else \"\",\n",
    "        \"Category\" : category if category else \"\",\n",
    "        \"Employment\" : employment if employment else \"\",\n",
    "        \"Experience\" : experience if experience else \"\",\n",
    "        \"English\" : english if english else \"\",\n",
    "        \"Domain\" : domain if domain else \"\",\n",
    "        \"Description\" : description if description else \"\",\n",
    "    }\n",
    "\n",
    "def make_output_path(filename:str) -> str:\n",
    "    output_path, output_dir = get_output_paths(filename)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file = open(output_path, \"w\")\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=fill_job_template())\n",
    "    csv_writer.writeheader()\n",
    "    file.close()\n",
    "    return output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(html, url):\n",
    "  try:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    company_name = soup.select_one('.job-details--title').text.strip()\n",
    "    job_title = soup.select_one('div.detail--title-wrapper h1').get_text(strip=True, separator=';').split(';')[0]\n",
    "    salary_elem = soup.select_one('div.detail--title-wrapper h1 span')\n",
    "    salary = salary_elem.text.replace(\"від\", \"\").replace(\"$\", \"\").strip() if salary_elem else \"\"\n",
    "    location = soup.select_one('div.job-additional-info--item-text span.location-text').text.strip().replace(\"\\n\", \" \")\n",
    "    description = soup.select_one('div.mb-4').text.strip()\n",
    "    views = int(soup.select_one('span.bi.bi-eye').next_sibling.strip().split()[0])\n",
    "    responses = int(soup.select_one('span.bi.bi-people-fill').next_sibling.strip().split()[0])\n",
    "    category = soup.select_one('li.job-additional-info--item:contains(\"Категорія:\")').find_all('span')[1].text.strip().replace(\"\\n\", \" \")\n",
    "    \n",
    "    employment_selector = \", \".join(f\"li.job-additional-info--item:contains(\\\"{x}\\\")\" for x in [\"Office\", \"Remote\", \"Гібридна\", \"віддалено\", \"офіс\"])\n",
    "    employment = soup.select_one(f\"{employment_selector} div\").text.strip().replace(\"\\n\", \"\")\n",
    "    experience_text = soup.select_one('li.job-additional-info--item:contains(\"досвіду\") div').text.strip().split()\n",
    "    experience = 0 \n",
    "    try:\n",
    "       experience = int(experience_text[0])\n",
    "    except:\n",
    "       experience = 0\n",
    "    english_elem = soup.select_one('li.job-additional-info--item:contains(\"Англійська\") div')\n",
    "    english = english_elem.text.strip().replace(\"\\n\", \" \") if english_elem else \"\"\n",
    "    \n",
    "    domain_elem = soup.select_one('li.job-additional-info--item:contains(\"Домен\") div')\n",
    "    domain = domain_elem.text.strip().replace(\"\\n\", \" \") if domain_elem else \"\"\n",
    "\n",
    "    return fill_job_template(\n",
    "      url,\n",
    "      company_name,\n",
    "      job_title,\n",
    "      salary,\n",
    "      location,\n",
    "      description,\n",
    "      views,\n",
    "      responses,\n",
    "      category,\n",
    "      employment,\n",
    "      experience,\n",
    "      english,\n",
    "      domain\n",
    "    )\n",
    "  except Exception as e:\n",
    "      print('parse_html',url, e )\n",
    "\n",
    "\n",
    "def format_output(result: dict):\n",
    "   return \"\\n\".join([f\"{k}:{v}\" for k,v in result.items()]) + \"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    constants = importlib.import_module(\"knowledge-bases.hw1.constants\")\n",
    "except:\n",
    "    print('')\n",
    "from constants import JOBS_URL, URL\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def write_job_links(output_path, job_links):\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(job_links)\n",
    "\n",
    "def write_job(output_path, job_info):\n",
    "    with open(output_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fill_job_template())\n",
    "        csv_writer.writerow(job_info)\n",
    "\n",
    "def get_pages_urls():\n",
    "    response = requests.get(JOBS_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    last_li = soup.find('ul', class_='pagination pagination_with_numbers').find_all('li')[-2]\n",
    "    number_of_pages = int(last_li.text.strip())\n",
    "    urls = [f\"{JOBS_URL}/?page={page}\" for page in range(1, number_of_pages + 1)]\n",
    "    return urls\n",
    "\n",
    "def get_job_urls(url):\n",
    "    print(f\"processing\", url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    job_links = soup.select('.job-list-item__link')\n",
    "    return [f\"{URL}{link['href']}\" for link in job_links]\n",
    "\n",
    "\n",
    "def get_job_urls_multi():\n",
    "    pages = get_pages_urls()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        job_urls = list(executor.map(get_job_urls, pages))\n",
    "    job_urls = [url for sublist in job_urls for url in sublist]\n",
    "    return job_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import importlib  \n",
    "try:\n",
    "    constants = importlib.import_module(\"knowledge-bases.hw1.constants\")\n",
    "except:\n",
    "    print('')\n",
    "from constants import OUTPUT_FILE_NAME\n",
    "\n",
    "def process_job_page(url, output_path):\n",
    "        print(f\"Processing {url}\")\n",
    "        response = requests.get(url)\n",
    "        output = parse_html(response.text, url)\n",
    "        write_job(output_path, output)\n",
    "\n",
    "\n",
    "def write_job_links():\n",
    "    with open(get_output_paths(\"jobs.txt\")[0], \"w\") as file:\n",
    "        job_urls = get_job_urls_multi()\n",
    "        file.writelines([f\"{url[:-1]}\\n\" for url in job_urls])\n",
    "\n",
    "def read_job_links():\n",
    "    with open(get_output_paths(\"jobs.txt\")[0], \"r\") as file:\n",
    "        return [url.strip() for url in file.readlines()]\n",
    "\n",
    "def main():\n",
    "    LIMIT = 1000\n",
    "    output_path = make_output_path(OUTPUT_FILE_NAME)\n",
    "    job_urls = read_job_links()\n",
    "    job_urls = job_urls[:LIMIT] if LIMIT else job_urls\n",
    "    print(f\"Processing {len(job_urls)} pages: {', '.join(job_urls[:3])}\\n...\")\n",
    "    # for job_url in job_urls:\n",
    "    #     process_job_page(job_url, output_path)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(process_job_page, job_urls, [output_path] * len(job_urls))\n",
    "    print(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
